---
type: note
category: development
created: 2025-09-18 11:53
modified: 2025-09-18 11:53
tags: ['model', 'programming']
status: draft
source: inbox_processing
original_file: 20250918_114949_rss_Qwen3-Next-80B-A3B_____Who_needs_legs__.txt
---

# Qwen3-Next-80B-A3B

## summary
The Qwen3-Next-80B-A3B model is a new generation of Qwen3-Next models, which can approach the performance of flagship models with only 80 billion parameters. It has some unique features like hybrid training and parameter management.

## content
RSS Feed: Simon Willison's Weblog
Source: https://simonwillison.net/atom/everything/
Link: https://simonwillison.net/2025/Sep/12/qwen3-next/#atom-everything

Qwen3-Next-80B-A3B: üêßü¶© Who needs legs?!

Qwen3-Next-80B-A3B Qwen announced two new models via their Twitter account (and here's their blog): Qwen3-Next-80B-A3B-Instruct and Qwen3-Next-80B-A3B-Thinking. They make some big claims on performance: Qwen3-Next-80B-A3B-Instruct approaches our 235B flagship. Qwen3-Next-80B-A3B-Thinking outperforms Gemini-2.5-Flash-Thinking. The name "80B-A3B" indicates 80 billion parameters of which only 3 billion are active at a time. You still need to have enough GPU-accessible RAM to hold all 80 billion in memory at once but only 3 billion will be used for each round of inference, which provides a significant speedup in responding to prompts. More details from their tweet: 80B params, but only 3B activated per token ‚Üí 10x cheaper training, 10x faster inference than Qwen3-32B.(esp. @ 32K+ context!) Hybrid Architecture: Gated DeltaNet + Gated Attention ‚Üí best of speed &amp; recall Ultra-sparse MoE: 512 experts, 10 routed + 1 shared Multi-Token Prediction ‚Üí turbo-charged speculative decoding Beats Qw...

## Scraped from https://simonwillison.net/atom/everything/
<?xml version="1.0" encoding="utf-8"?>
<feed xml:lang="en-us" xmlns="http://www.w3.org/2005/Atom"><title>Simon Willison's Weblog</title><link href="http://simonwillison.net/" rel="alternate"/><link href="http://simonwillison.net/atom/everything/" rel="self"/><id>http://simonwillison.net/</id><updated>2025-09-17T23:53:38+00:00</updated><author><name>Simon Willison</name></author><entry><title>Anthropic: A postmortem of three recent issues</title><link href="https://simonwillison.net/2025/Sep/17/anthropic-postmortem/#atom-everything" rel="alternate"/><published>2025-09-17T23:53:38+00:00</published><updated>2025-09-17T23:53:38+00:00</updated><id>https://simonwillison.net/2025/Sep/17/anthropic-postmortem/#atom-everything</id><summary type="html">
    
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues"&gt;Anthropic: A postmortem of three recent issues&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
Anthropic had a very bad month in terms of model reliability:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Between August and early September, three infrastructure bugs intermittently degraded Claude's response quality. We've now resolved these issues and want to explain what happened. [...]&lt;/p&gt;
&lt;p&gt;To state it plainly: We never reduce model quality due to demand, time of day, or server load. The problems our users reported were due to infrastructure bugs alone. [...]&lt;/p&gt;
&lt;p&gt;We don't typically share this level of technical detail about our infrastructure, but the scope and complexity of these issues justified a more comprehensive explanation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm really glad Anthropic are publishing this in so much detail. Their reputation for serving their models reliably has taken a notable hit.&lt;/p&gt;
&lt;p&gt;I hadn't appreciated the additional complexity caused by their mixture of different serving platforms:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We deploy Claude across multiple hardwar...


## Scraped from https://simonwillison.net/2025/Sep/12/qwen3-next/#atom-everything
Qwen3-Next-80B-A3B: üêßü¶© Who needs legs?! Simon Willison‚Äôs Weblog Subscribe Qwen3-Next-80B-A3B. Qwen announced two new models via their Twitter account (and here's their blog): Qwen3-Next-80B-A3B-Instruct and Qwen3-Next-80B-A3B-Thinking. They make some big claims on performance: Qwen3-Next-80B-A3B-Instruct approaches our 235B flagship. Qwen3-Next-80B-A3B-Thinking outperforms Gemini-2.5-Flash-Thinking. The name "80B-A3B" indicates 80 billion parameters of which only 3 billion are active at a time. You still need to have enough GPU-accessible RAM to hold all 80 billion in memory at once but only 3 billion will be used for each round of inference, which provides a significant speedup in responding to prompts. More details from their tweet: 80B params, but only 3B activated per token ‚Üí 10x cheaper training, 10x faster inference than Qwen3-32B.(esp. @ 32K+ context!) Hybrid Architecture: Gated DeltaNet + Gated Attention ‚Üí best of speed &amp; recall Ultra-sparse MoE: 512 experts, 10 routed + 1 shared Multi-Token Prediction ‚Üí turbo-charged speculative decoding Beats Qwen3-32B in perf, rivals Qwen3-235B in reasoning &amp; long-context The models on Hugging Face are around 150GB each so I decided to try them out via OpenRouter rather than on my own laptop (Thinking, Instruct). I'm used my llm-openrouter plugin. I installed it like this: llm install llm-openrouter llm keys set openrouter # paste key here Then found the model IDs with this command: llm models -q next Which output: OpenRouter: openrouter/qwen/qwen3-next-80b-a3b-thinking OpenRouter: openrouter/qwen/qwen3-next-80b-a3b-instruct I have an LLM prompt template saved called pelican-svg which I created like this: llm "Generate an SVG of a pelican riding a bicycle" --save pelican-svg This means I can run my pelican benchmark like this: llm -t pelican-svg -m openrouter/qwen/qwen3-next-80b-a3b-thinking Or like this: llm -t pelican-svg -m openrouter/qwen/qwen3-next-80b-a3b-instruct Here's the thinking model output (exported w...


## connections
- processed from phone shortcut
